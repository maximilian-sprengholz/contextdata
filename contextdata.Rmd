---
output: html_document
---

# Workshop räumliche Kontextdaten

Jupyter R Studio environment mit Beispielen/Übungen zum automatisierten Ziehen von räumlichen Kontextdaten für Deutschland aus verschiedenen Quellen.

```{r, setup}
# packages
library(dplyr)
library(ggplot2)
library(readr)
library(stringr)
library(tibble)
library(terra)
library(tidyterra)

# shapefiles
Sys.setenv(PROJ_LIB = "/srv/conda/envs/notebook/share/proj") # auskommentieren wenn lokal
kre22 <- vect("data/kre22/kre22.shp") # Kreise Berlin, Brandenburg, Sachsen 2022
kre05 <- project(vect("data/kre05/kre05.shp"), crs(kre22)) # 2005
```



## Google

### API Zugang

- Ihr braucht einen API key von [Google Cloud](https://cloud.google.com/)
- Neue User bekommen free credits (vor 1 Jahr ~ 300 Euro) und einige Services erlauben limitierte kostenlose Anfragen pro Tag, aber generell kostet jede Anfrage $

```{r}
library(googleway) # access google API
key <- ""
set_key(key = key)
```

### Google Places Beispiel

- Interesse: lokale Gesundheitsversorgung
- Vorhandene Daten: Koordinaten zum Wohnort einer Person
- Abfrage: Krankenhäuser in der Umgebung

```{r}
# example coordinates: DeZIM
coords <- c(52.50960542566534, 13.387616284657216) # no standard; Google = lat lng
res <- google_places(
  location = coords,
  keyword = "Krankenhaus",
  radius = 5000, # 5km Umkreis; aber Achtung: nicht strikt umgesetzt
  language = "de"
  )
print(names(res$results))
print(nrow(res$results)) # max. 20/page; next: new request with page_token arg
# res <- google_places(
#   location = coords,
#   keyword = "Krankenhaus",
#   radius = 5000,
#   language = "de",
#   page_token = res$next_page_token
#   )
```

```{r}
res <- as.data.frame(
  cbind(
    res$result[c("name", "rating", "types", "vicinity")],
    res$result$geometry$location
    )
  )
print(res)
```

```{r}
google_p <- vect(res, geom = c("lng", "lat"), crs="EPSG:4326")
person_p <- vect(data.frame(lat=coords[1], lng=coords[2]), geom=c("lng", "lat"), crs="EPSG:4326")
person_5km_radius <- buffer(person_p, width=5000)
ggplot() +
  geom_spatvector(data = kre22 %>% filter(SN_L == "11"), fill = "white") + # Berlin
  geom_spatvector(data = project(google_p, crs(kre22))) + # Google results
  geom_spatvector(data = project(person_p, crs(kre22)), color = "purple") + # person
  geom_spatvector(data = project(person_5km_radius, crs(kre22)), color = "purple", fill = NA) # radius
```

Wie ginge es weiter?
- z.B. räumliche Distanz zum nächsten Krankenhaus messen
- Luftlinie vs. real gegeben Verkehrsmittel (wieder über Google API)

__Ausprobieren: Google details über das DeZIM__

- Sucht nach dem DeZIM über:
  ```r
  # 1. Sucht nach dem DeZIM über:
  res <- google_places(
    search_string = "Suchstring",
    key = key
    )
  ```
- Schaut euch an, ob ihr das richtige gefunden habt:
  ```r
  print(res$result[c("name", "formatted_address", "place_id")])
  ```

- Ja? Dann könnt ihr eine Detailsuche starten über die `place_id` angebt:
  ```r
  det <- google_place_details(
    place_id = "DeZIM_place_id",
    key = key
    )
  ```
- Schaut euch die Details an. Was steckt alles drin?
  ```r
  print(names(det$result))
  ```
- Was sind eure letzten Reviews?



## INKAR

Beispiel Arbeitslosenquote auf Kreislevel.
```{r}
library(bonn) # access INKAR API 
themes <- get_themes(geography = "KRE") # ARS: "GEM", "GVB", "KRE", "RBZ"
tibble(themes)
```
Ist hier unter "Allgemein", also `011`.
```{r}
vars <- get_variables(theme = "011", geography = "KRE")
tibble(vars)
```
"Gruppe" entspricht der zu ziehenden Variable, also `12`.
```{r}
alq_kre <- get_data(variable = "12", geography = "KRE")
print(tibble(alq_kre)) # Seit 1998!
print(length(unique(alq_kre$Schlüssel))) # 400 Kreise -> Gebietsstand 2022
print(max(alq_kre$Zeit)) # aber hier letztes Jahr 2021
```
Kann über `Schlüssel` an den Kreis `SpatVector` gemerged und dann geplottet werden für z.B. 2021:
```{r}
kre22 <- merge(
  kre22, 
  alq_kre %>% filter(Zeit == "2021") %>% rename(alq = Wert), # bestenfalls umbenennen!
  all.x = TRUE, by.x = "AGS", by.y = "Schlüssel"
  )
# plot
ggplot() +
  geom_spatvector(data = kre22, aes(fill = alq)) +
  scale_fill_whitebox_c(palette = "pi_y_g", direction=-1) +
  labs(
    fill = "Arbeitslosenquote",
    title = "Arbeitslosenquote nach Kreis, 2021"
  )
```

__Ausprobieren:__

- Schaut euch noch mal die `themes` an. Wir würden gern die Dichte an Ärzt:innen pro 10000 Einwohner:innen im Landkreis wissen.
  ```r
  print(themes, n = 50, na.print = "")
  ```
- Nutzt nun denselben Ablauf wie oben, um die entsprechenden Variablen zu finden und über `get_data` die Daten zu ziehen.
- Merged die Daten analog zu oben nacheinander an `kre22` und plottet
- Wie schneidet Berlin ab im Vergleich zu den anderen Kreisen?



## DWD Beispiel

Tägliche Niederschlagsdaten (1x1 km, 2024).
```{r}
hyras <- "https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/"
file <- "pr_hyras_1_2024_v5-0_de.nc"
download.file(paste0(hyras, file), paste0("data/", file))
precip_r <- rast(paste0("data/", file))
print(precip_r)
```
Nur weil es so schön ist:
```{r}
recent <- dim(precip_r)[3]
plot(precip_r[[recent]], main = paste("Niederschlagssumme (mm)", time(precip_r[[recent]])))
```
Niederschlag am DeZIM 2024:
```{r}
precip_dezim <- extract(precip_r, project(person_p, crs(precip_r)))
precip_dezim <- as.data.frame(t(precip_dezim[, 2:length(precip_dezim)]))
ggplot(precip_dezim, aes(x = time(precip_r), y = V1)) +
  geom_bar(stat = "identity") + ylab("mm") + xlab("Tag")
```



## Harmonisierung

Das Beispiel zeigt die Veränderung des AGS durch Zusammenlegung verschiedener Kreise in Sachsen zwischen 2005 und 2009, welche zu einem Zeitreihenbruch führen in den Daten zur Wahlbeteiligung zu den Bundestagswahlen 1998 - 2017. Dieser Bruch kann durch Crosswalks behoben werden, hier umgesetzt mit dem R-package `ags` (aus der Package [Vignette](https://www.moritz-marbach.com/ags/articles/ags.html) ist auch das Beispiel).

Zeitreihenbruch:

```{r}
library(ags) # crosswalk AGS
# Wahlbeteilung Bundestagswahl: Luecke 2005 -> 2009
data(btw_sn)
btw_sn <- btw_sn %>% filter(year >= 1998)
ggplot(btw_sn, aes(year, (valid/voters)*(100), group=district)) + 
  geom_line() + geom_point() + ylab("Turnout (in %)") + xlab("Year")
```

AGS 2005 und 2022 im Vergleich:

```{r}
ggplot() +
  geom_spatvector(data = kre05 %>% filter(str_detect(AGS, "^14")), color = "purple", fill = "white") +
  geom_spatvector(data = kre22 %>% filter(SN_L == "14"), fill = NA, lwd = 0.6)
```

Nach Crosswalk:

```{r}
# crosswalk (geht hier bis 2020, aber danach gab es keine Veraenderung)
btw_sn_ags20 <- xwalk_ags(
  data=btw_sn, 
  ags="district", 
  time="year", 
  xwalk="xd20", 
  variables=c("voters", "valid"), 
  weight="pop")
ggplot(btw_sn_ags20, aes(year, (valid/voters)*100, group=ags20)) + 
  geom_line() + geom_point() + ylab("Turnout (in %)") + xlab("Year")

```

Diese Harmonisierung ist immer fehlerbehaftet, wenn Einheiten nicht eindeutig zugeordnet werden können. Aber selbst bei eindeutiger Zuordnung gehen Informationen verloren, welche mglw. interessant sind. In bereits harmonisierten Daten (z.B. INKAR), gibt es keine Möglichkeit, historische Heterogenitäten nachzuvollziehen.

In unserem Beispiel:

```{r}
# merge 2005 turnout to shapefiles
t1 <- merge(
  kre05 %>% mutate(AGS = substr(AGS, 1, 5)), 
  btw_sn %>% filter(year == 2005) %>% mutate(turnout = (valid/voters)*100), 
  all.x = FALSE, by.x = "AGS", by.y = "district"
  )
t2 <- merge(
  kre22, 
  btw_sn_ags20 %>% filter(year == 2005) %>% mutate(turnout = (valid/voters)*100), 
  all.x = FALSE, by.x = "AGS", by.y = "ags20"
  )
# plot
ggplot() +
  geom_spatvector(data = t1, aes(fill = turnout)) +
  scale_fill_whitebox_c(palette = "pi_y_g", limits=c(70,80)) +
  labs(
    fill = "Turnout",
    title = "Wahlbeteiligung BTW 2005",
    subtitle = "Sachsen nach Kreisen"
  )
ggplot() +
  geom_spatvector(data = t2, aes(fill = turnout)) +
  scale_fill_whitebox_c(palette = "pi_y_g", limits=c(70,80)) +
  labs(
    fill = "Turnout",
    title = "Wahlbeteiligung BTW 2005",
    subtitle = "Sachsen nach Kreisen von 2020"
  )
```


